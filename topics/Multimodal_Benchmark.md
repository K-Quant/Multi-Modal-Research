Multimodal Benchmarks
======

### Benchmark

1. MultiBench [[Paper](https://arxiv.org/pdf/2107.07502.pdf)][[Code](https://github.com/pliang279/MultiBench)]: a benchmark for multimodal learning spanning 15 datasets, 10 modalities, 20 prediction tasks, and 6 research areas. MultiBench provides an automated end-to-end machine learning pipeline, and offers evaluation methodology to study (1) generalization, (2) time and space complexity, and (3) modality robustness.

2. FedMultimodal [[Paper](https://arxiv.org/pdf/2306.09486.pdf)][[Code](https://github.com/usc-sail/fed-multimodal)]: is an open source project for researchers exploring multimodal applications in Federated Learning setup.


3. Benchmarking Multimodal AutoML for Tabular Data with Text Fields [[Paper](https://arxiv.org/pdf/2111.02705.pdf)][[Code](https://github.com/sxjscience/automl_multimodal_benchmark)]


4. Multimodal Sentiment Analysis Benchmark on CMU-MOSI [[Link](https://paperswithcode.com/sota/multimodal-sentiment-analysis-on-cmu-mosi)]


### High quality multi-modal dataset in Kaggle
1. Multimodal Hate Speech(text+image)[[link](https://www.kaggle.com/datasets/victorcallejasf/multimodal-hate-speech)]
2. Extended Wikipedia Multimodal Dataset(text+image)[[link](https://www.kaggle.com/datasets/jacksoncrow/extended-wikipedia-multimodal-dataset)]
3. Tufts Face Database(7 image modalities: visible, near-infrared, thermal, computerized sketch, video, LYTRO and 3D images)[[link](https://www.kaggle.com/datasets/kpvisionlab/tufts-face-database)]
4. Multi-modal MIREX Emotion Dataset(voice+text+midi)[[link](https://www.kaggle.com/datasets/imsparsh/multimodal-mirex-emotion-dataset)]
5. Lifesnaps Fitbit dataset(different health data)[[link](https://www.kaggle.com/datasets/skywescar/lifesnaps-fitbit-dataset)]
6. Multimodal EmotionLines Dataset(MELD)(dialogue+audio+visual)[[link](https://www.kaggle.com/datasets/zaber666/meld-dataset)]
7. arXiv Dataset[[link](https://www.kaggle.com/datasets/Cornell-University/arxiv)]
8. Ryerson Emotion Database(audio+visual)[[link](https://www.kaggle.com/datasets/ryersonmultimedialab/ryerson-emotion-database)]
9. Enhanced MovieLens 100K(video+table+text...)[[link](https://www.kaggle.com/datasets/lamarockzz/enhanced-movielens-100k)]

### Other open-source multi-modal dataset 
1. VQA v2.0: The Visual Question Answering dataset has a collection of images and questions about them that are open-ended and can be answered in multiple ways.[[link](https://visualqa.org/)]
2. Multi30K: A dataset for Multimodal Machine Translation, contains 31,014 images from Flickr and 145,000 sentences in English and German.[[link](https://multi30kdataset.readthedocs.io/en/latest/)]
3. CMU-MOSEI: Multimodal Sentiment Analysis dataset contains sentiment annotations and labels for a large number of video clips from the Internet.[[link](http://immortal.multicomp.cs.cmu.edu/raw_datasets/)]
4. REDDIT-MULTI-5K: A dataset from the Reddit platform, which is a rich source of naturally occurring multi-modal (text, metadata and image) data.[[link](http://ama.liglab.fr/data/)]
5. MuSe-CaR: Multimodal Sentiment Analysis in Real-life Media dataset contains real-life data from YouTube videos including the audio, visual and verbal modalities.[[link](https://www.musecar.com/)]
6. MS-COCO (Microsoft Common Objects in Context): A dataset for object detection, segmentation, and captioning. It includes images and text captions that describe the images. [[link](https://cocodataset.org/#home)]
7. Flickr30k: A dataset consisting of 31,000 images and 5 captions per image. It combines images and text. [[link](http://shannon.cs.illinois.edu/DenotationGraph/)]
8. IMDB-WIKI: A large dataset of celebrity faces with information about their age, gender, and name. It includes images and categorical data. [[link](https://data.vision.ee.ethz.ch/cvl/rrothe/imdb-wiki/)]
9. MNLI (Multi-Genre Natural Language Inference): A dataset for textual entailment that includes text pairs from multiple genres and sources. [[link](https://cims.nyu.edu/~sbowman/multinli/)]
10. TabFact: A large-scale dataset consisting of 16k Wikipedia tables and 118k natural language sentences that entail a true/false judgment. It combines tabular data and text. [[link](https://github.com/wenhuchen/Table-Fact-Checking)]
11. ImageCLEF: A benchmark series that offers tasks for multimodal classification, where images are paired with text and metadata. [[link](https://www.imageclef.org/)]
12. CUHK-PEDES: A dataset for person search with natural language description, containing images of people and their textual descriptions. [[link](https://github.com/ShuangLI59/Person_Search_with_Natural_Language_Description)]
13. CLEVR: A dataset for visual reasoning tasks. It contains synthetic images, scene graphs, and questions about the images. [[link](https://cs.stanford.edu/people/jcjohns/clevr/)]
14. MM-IMDB (Multi-Modal IMDB): A dataset for movie genre classification. It combines text (movie plots) and images (movie posters). [[link](https://github.com/MIRALab-USTB/MM-IMDB)]
15. Recipe1M: A dataset containing over one million cooking recipes and 800k food images, combining text (recipes) and images (food). [[link](http://pic2recipe.csail.mit.edu/)]
16. Synthetic Data: [*DAG Data](https://github.com/Lantian72/MM-DAG/blob/main/sim.py)

17. Satellite feature sources: [Meta](https://dataforgood.facebook.com/dfg/tools) - Population, Mobility, Demographics; [OpenStreetMap](https://www.kaggle.com/datasets/bigquery/geo-openstreetmap) - Infrasturcture; [OpenCelliD](https://www.opencellid.org/downloads.php) - Connectivity.

18. Social Media: [Twitter100k](https://github.com/huyt16/Twitter100k) (100,000 text-image pairs); [HFUT-mmdata](http://scholarhub.cn/ScholarHubProject/MMTM/HFUT-mmdata.zip) (74,364 text-image pairs)

19. Sentiment Analysis Data: [CMU-MOSI](http://multicomp.cs.cmu.edu/resources/cmu-mosi-dataset/); [CMU-MOSEI](http://multicomp.cs.cmu.edu/resources/cmu-mosei-dataset/).