Note: Taxonomy of six core technical challenges in Multimodal Machine Learning: Representation, Alignment, Reasoning, Generation, Transference, and Quantification.[cite from](https://arxiv.org/abs/2209.03430)

# AAAI 
## Image & Text

[Fusion] MMTN: Multi-Modal Memory Transformer Network for Image-Report Consistent Medical Report Generation.(2023) [Link](https://ojs.aaai.org/index.php/AAAI/article/view/25100)

[Representation Learning/Transference] Open-Vocabulary Multi-Label Classification via Multi-Modal Knowledge Transfer.(2023) 

Multi-Modality Deep Network for Extreme Learned Image Compression. (2023)[Link](https://ojs.aaai.org/index.php/AAAI/article/view/25184) 

You Only Infer Once: Cross-Modal Meta-Transfer for Referring Video Object Segmentation. (2022) [Link](https://ojs.aaai.org/index.php/AAAI/article/view/20017)

Show Your Faith: Cross-Modal Confidence-Aware Network for Image-Text Matching. (2022) [Link](https://ojs.aaai.org/index.php/AAAI/article/view/20235)

MAGIC: Multimodal relational Graph adversarial inference for Diverse and Unpaired Text-Based Image Captioning. (2022) [Link](https://ojs.aaai.org/index.php/AAAI/article/view/20243)

Hierarchical Cross-Modality Semantic Correlation Learning Model for Multimodal Summarization. (2022) [Link](https://ojs.aaai.org/index.php/AAAI/article/view/21422)

UniMS: A Unified Framework for Multimodal Summarization with Knowledge Distillation. (2022) [Link](https://ojs.aaai.org/index.php/AAAI/article/view/21431)

Sentiment and Emotion-Aware Multi-Modal Complaint Identification. (2022) [Link](https://ojs.aaai.org/index.php/AAAI/article/view/21476)

Are Vision-Language Transformers Learning Multimodal Representations? A Probing Perspective. (2022) [Link](https://ojs.aaai.org/index.php/AAAI/article/view/21375)

MNER-QG: An End-to-End MRC Framework for Multimodal Named Entity Recognition with Query Grounding.(2023) [Link](https://ojs.aaai.org/index.php/AAAI/article/view/25971)

## Video/Audio & Text
[Alignment] Tagging before Alignment: Integrating Multi-Modal Tags for Video-Text Retrieval.(2023) [Link](https://ojs.aaai.org/index.php/AAAI/article/view/25113)
[Translation] Efficient End-to-End Video Question Answering with Pyramidal Multimodal Transformer. (2023)

## Audio & Visual
[Representation Learning] Self-Supervised Audio-Visual Representation Learning with Relaxed Cross-Modal Synchronicity. (2023) [Link](https://ojs.aaai.org/index.php/AAAI/article/view/26162)

## Verbal (text) and nonverbal cues
[Fusion] PATRON: Perspective-Aware Multitask Model for Referring Expression Grounding Using Embodied Multimodal Cues.(2023) [Link](https://ojs.aaai.org/index.php/AAAI/article/view/25177)

## Related to Graph
Multi-Modal Knowledge Hypergraph for Diverse Image Retrieval. (2023) [Link](https://ojs.aaai.org/index.php/AAAI/article/view/25445)

## Others  
[Representation Learning] M3AE: Multimodal Representation Learning for Brain Tumor Segmentation with Missing Modalities. (2023)
Learning Polysemantic Spoof Trace: A Multi-Modal Disentanglement Network for Face Anti-spoofing. (2023) [Link](https://ojs.aaai.org/index.php/AAAI/article/view/25219)

## Specific Application
[Representation Learning] Sparse Maximum Margin Learning from Multimodal Human Behavioral Patterns. (2023) [Link](https://ojs.aaai.org/index.php/AAAI/article/view/25676)
[Representation Learning] M3AE: Multimodal Representation Learning for Brain Tumor Segmentation with Missing Modalities. (2023)
Heterogeneous Graph Learning for Multi-Modal Medical Data Analysis. (2023) [Link](https://ojs.aaai.org/index.php/AAAI/article/view/25643)
Causal Conditional Hidden Markov Model for Multimodal Traffic Prediction. (2023) [Link](https://ojs.aaai.org/index.php/AAAI/article/view/25619)
 



## TODO


Just Noticeable Visual Redundancy Forecasting: A Deep Multimodal-Driven Approach. (2023) [Link](https://ojs.aaai.org/index.php/AAAI/article/view/25399)



TOT: Topology-Aware Optimal Transport for Multimodal Hate Detection. (2023) [Link](https://ojs.aaai.org/index.php/AAAI/article/view/25614)


Mutual-Enhanced Incongruity Learning Network for Multi-Modal Sarcasm Detection. (2023) [Link](https://ojs.aaai.org/index.php/AAAI/article/view/26138)

Accommodating Audio Modality in CLIP for Multimodal Processing. (2023) [Link](https://ojs.aaai.org/index.php/AAAI/article/view/26153)


i-Code: An Integrative and Composable Multimodal Learning Framework. (2023) [Link](https://ojs.aaai.org/index.php/AAAI/article/view/26290)
Joint Multimodal Entity-Relation Extraction Based on Edge-Enhanced Graph Alignment Network and Word-Pair Relation Tagging. (2023) [Link](https://ojs.aaai.org/index.php/AAAI/article/view/26309)
Multi-Level Confidence Learning for Trustworthy Multimodal Classification. (2023) [Link]()
Bayesian Cross-Modal Alignment Learning for Few-Shot Out-of-Distribution Generalization. (2023) [Link](https://ojs.aaai.org/index.php/AAAI/article/view/26355)
Cross-Modal Distillation for Speaker Recognition. (2023) [Link](https://ojs.aaai.org/index.php/AAAI/article/view/26525)
Explaining (Sarcastic) Utterances to Enhance Affect Understanding in Multimodal Dialogues. (2023) [Link](https://ojs.aaai.org/index.php/AAAI/article/view/26526)
SPRING: Situated Conversation Agent Pretrained with Multimodal Questions from Incremental Layout Graph. (2023) [Link](https://doi.org/10.1609/aaai.v37i11.26562)
See How You Read? Multi-Reading Habits Fusion Reasoning for Multi-Modal Fake News Detection. (2023) [Link](https://ojs.aaai.org/index.php/AAAI/article/view/26609)
MPMQA: Multimodal Question Answering on Product Manuals. (2023) [Link](https://ojs.aaai.org/index.php/AAAI/article/view/26634)
FakeSV: A Multimodal Benchmark with Rich Social Context for Fake News Detection on Short Video Platforms. (2023) [Link](https://ojs.aaai.org/index.php/AAAI/article/view/26689)
MuMIC - Multimodal Embedding for Multi-Label Image Classification with Tempered Sigmoid. (2023) [Link](https://ojs.aaai.org/index.php/AAAI/article/view/26850)
Multimodal Propaganda Processing. (2023) [Link](https://ojs.aaai.org/index.php/AAAI/article/view/26792)
Event-Image Fusion Stereo Using Cross-Modality Feature Propagation. (2022) [Link](https://ojs.aaai.org/index.php/AAAI/article/view/19923)
MuMu: Cooperative Multitask Learning-Based Guided Multimodal Fusion. (2022) [Link](https://ojs.aaai.org/index.php/AAAI/article/view/19988)
Cross-Modal Object Tracking: Modality-Aware Representations and a Unified Benchmark. (2022) [Link](https://ojs.aaai.org/index.php/AAAI/article/view/20016)
Multi-Modal Perception Attention Network with Self-Supervised Learning for Audio-Visual Speaker Tracking. (2022) [Link](https://ojs.aaai.org/index.php/AAAI/article/view/20035)
Visual Sound Localization in the Wild by Cross-Modal Interference Erasing. (2022) [Link](https://ojs.aaai.org/index.php/AAAI/article/view/20073)
TVT: Three-Way Vision Transformer through Multi-Modal Hypersphere Learning for Zero-Shot Sketch-Based Image Retrieval. (2022) [Link](https://ojs.aaai.org/index.php/AAAI/article/view/20136)
Interact, Embed, and EnlargE: Boosting Modality-Specific Representations for Multi-Modal Person Re-identification. (2022) [Link](https://ojs.aaai.org/index.php/AAAI/article/view/20165)
Multi-Modal Answer Validation for Knowledge-Based VQA. (2022) [Link](https://ojs.aaai.org/index.php/AAAI/article/view/20174)
Cross-Modal Mutual Learning for Audio-Visual Speech Recognition and Manipulation. (2022) [Link](https://ojs.aaai.org/index.php/AAAI/article/view/20210)
Cross-Modal Federated Human Activity Recognition via Modality-Agnostic and Modality-Specific Representation Learning. (2022) [Link](https://ojs.aaai.org/index.php/AAAI/article/view/20213)
Event-Aware Multimodal Mobility Nowcasting. (2022) [Link](https://ojs.aaai.org/index.php/AAAI/article/view/20342)
Online Enhanced Semantic Hashing: Towards Effective and Efficient Retrieval for Streaming Multi-Modal Data. (2022) [Link](https://ojs.aaai.org/index.php/AAAI/article/view/20346)
Multimodal Adversarially Learned Inference with Factorized Discriminators. (2022) [Link](https://ojs.aaai.org/index.php/AAAI/article/view/20580)
Multi-Head Modularization to Leverage Generalization Capability in Multi-Modal Networks. (2022) [Link](https://ojs.aaai.org/index.php/AAAI/article/view/20698)
BM-NAS: Bilevel Multimodal Neural Architecture Search. (2022) [Link](https://ojs.aaai.org/index.php/AAAI/article/view/20872)
Tailor Versatile Multi-Modal Learning for Multi-Label Emotion Recognition. (2022) [Link](https://ojs.aaai.org/index.php/AAAI/article/view/20895)
D-vlog: Multimodal Vlog Dataset for Depression Detection. (2022) [Link](https://ojs.aaai.org/index.php/AAAI/article/view/21483)
Bi-level Dynamic Learning for Jointly Multi-modality Image Fusion and Beyond. (2023) [Link](https://www.ijcai.org/proceedings/2023/138)
Acoustic NLOS Imaging with Cross Modal Knowledge Distillation. (2023) [Link](https://www.ijcai.org/proceedings/2023/156)
3D Surface Super-resolution from Enhanced 2D Normal Images: A Multimodal-driven Variational AutoEncoder Approach. (2023) [Link](https://www.ijcai.org/proceedings/2023/175)
MM-PCQA: Multi-Modal Learning for No-reference Point Cloud Quality Assessment. (2023) [Link](https://www.ijcai.org/proceedings/2023/195)
A Multi-Modal Neural Geometric Solver with Textual Clauses Parsed from Diagram. (2023) [Link](https://www.ijcai.org/proceedings/2023/376)
Multi-Modality Deep Network for JPEG Artifacts Reduction. (2023) [Link](https://www.ijcai.org/proceedings/2023/429)
SSML-QNet: Scale-Separative Metric Learning Quadruplet Network for Multi-modal Image Patch Matching. (2023)[Link](https://www.ijcai.org/proceedings/2023/511)
Cross-Modal Global Interaction and Local Alignment for Audio-Visual Speech Recognition. (2023) [Link](https://www.ijcai.org/proceedings/2023/564)
AutoAlign: Pixel-Instance Feature Aggregation for Multi-Modal 3D Object Detection. (2022) [Link](https://www.ijcai.org/proceedings/2022/116)
Unsupervised Multi-Modal Medical Image Registration via Discriminator-Free Image-to-Image Translation. (2022) [Link](https://www.ijcai.org/proceedings/2022/117)
Representation Learning for Compressed Video Action Recognition via Attentive Cross-modal Interaction with Motion Enhancement. (2022) [Link](https://www.ijcai.org/proceedings/2022/148)
MFAN: Multi-modal Feature-enhanced Attention Networks for Rumor Detection. (2022) [Link](https://www.ijcai.org/proceedings/2022/335)
Cross-modal Representation Learning and Relation Reasoning for Bidirectional Adaptive Manipulation. (2022) [Link](https://www.ijcai.org/proceedings/2022/447)
MMT: Multi-way Multi-modal Transformer for Multimodal Learning. (2022) [Link](https://www.ijcai.org/proceedings/2022/480)
Recipe2Vec: Multi-modal Recipe Representation Learning with Graph Neural Networks. (2022) [Link](https://www.ijcai.org/proceedings/2022/482)
Unsupervised Voice-Face Representation Learning by Cross-Modal Prototype Contrast. (2022) [Link](https://www.ijcai.org/proceedings/2022/526)
Targeted Multimodal Sentiment Classification based on Coarse-to-Fine Grained Image-Target Matching. (2022) [Link](https://www.ijcai.org/proceedings/2022/622)



# IJCAI
## 2023
1. Bi-level Dynamic Learning for Jointly Multi-modality Image Fusion and Beyond.[Link](https://www.ijcai.org/proceedings/2023/138)
2. Acoustic NLOS Imaging with Cross Modal Knowledge Distillation.[Link](https://www.ijcai.org/proceedings/2023/156)
3. 3D Surface Super-resolution from Enhanced 2D Normal Images: A Multimodal-driven Variational AutoEncoder Approach.[Linke](https://www.ijcai.org/proceedings/2023/175)
4. MM-PCQA: Multi-Modal Learning for No-reference Point Cloud Quality Assessment. [Link](https://www.ijcai.org/proceedings/2023/195)
5. A Multi-Modal Neural Geometric Solver with Textual Clauses Parsed from Diagram.[Link](https://www.ijcai.org/proceedings/2023/376)
6. Multi-Modality Deep Network for JPEG Artifacts Reduction. [Link](https://www.ijcai.org/proceedings/2023/429)
7. SSML-QNet: Scale-Separative Metric Learning Quadruplet Network for Multi-modal Image Patch Matching.[Link](https://www.ijcai.org/proceedings/2023/511)
8. Cross-Modal Global Interaction and Local Alignment for Audio-Visual Speech Recognition.[Link](https://www.ijcai.org/proceedings/2023/564)
## 2022 
1. AutoAlign: Pixel-Instance Feature Aggregation for Multi-Modal 3D Object Detection. [Link](https://www.ijcai.org/proceedings/2022/116)
2. Unsupervised Multi-Modal Medical Image Registration via Discriminator-Free Image-to-Image Translation. [Link](https://www.ijcai.org/proceedings/2022/117)
3. Representation Learning for Compressed Video Action Recognition via Attentive Cross-modal Interaction with Motion Enhancement.[Link](https://www.ijcai.org/proceedings/2022/148)
4. MFAN: Multi-modal Feature-enhanced Attention Networks for Rumor Detection.[Link](https://www.ijcai.org/proceedings/2022/335)
5. Cross-modal Representation Learning and Relation Reasoning for Bidirectional Adaptive Manipulation.[Link](https://www.ijcai.org/proceedings/2022/447)
6. MMT: Multi-way Multi-modal Transformer for Multimodal Learning.[Link](https://www.ijcai.org/proceedings/2022/480)
7. Recipe2Vec: Multi-modal Recipe Representation Learning with Graph Neural Networks.[Link](https://www.ijcai.org/proceedings/2022/482)
8. Unsupervised Voice-Face Representation Learning by Cross-Modal Prototype Contrast.[Link](https://www.ijcai.org/proceedings/2022/526)
9. Targeted Multimodal Sentiment Classification based on Coarse-to-Fine Grained Image-Target Matching.[Link](https://www.ijcai.org/proceedings/2022/622)

# ICML
## 2023


1. Provable Dynamic Fusion for Low-Quality Multimodal Data.[Link](https://arxiv.org/pdf/2306.02050.pdf)
> * Introducing dynamic fusion mechanism to multimodal information fusion 
2. Calibrating Multimodal Learning.[Link](https://openreview.net/pdf?id=4PgzyLz6hi)
> * A novel regularization technique to calibrate the predictive confidence of previous methods.
3. π-Tuning: Transferring Multimodal Foundation Models with Optimal Multi-task Interpolation.[Link](https://proceedings.mlr.press/v202/wu23t/wu23t.pdf)
4. Improving Medical Predictions by Irregular Multimodal Electronic Health Records Modeling.[Link](https://proceedings.mlr.press/v202/zhang23v/zhang23v.pdf)
>* Medical Prediction
5. MEWL: Few-shot multimodal word learning with referential uncertainty.[Link](https://arxiv.org/pdf/2306.00503.pdf)
>* This paper introduces the MachinE Word Learning (MEWL) benchmark to assess how machines learn word meaning in grounded visual scenes.
6. VIMA: Robot Manipulation with Multimodal Prompts.[Link](https://proceedings.mlr.press/v202/jiang23b/jiang23b.pdf)
> * Multimodal application in generalist robot
7. Robustness in Multimodal Learning under Train-Test Modality Mismatch.[Link](https://proceedings.mlr.press/v202/mckinzie23a/mckinzie23a.pdf)
> * Robustness improvement in multimodal learning
8. Data Poisoning Attacks Against Multimodal Encoders.[Link](https://proceedings.mlr.press/v202/yang23f/yang23f.pdf)
> * Anti-attack in multimodal encoders
9. PaLM-E: An Embodied Multimodal Language Model.[Link](https://proceedings.mlr.press/v202/driess23a/driess23a.pdf)
> * This paper proposes embodied language models to directly incorporate real-world continuous sensor modalities into language models and thereby establish the link between words and percepts.
10. Grounding Language Models to Images for Multimodal Inputs and Outputs.[Link](https://openreview.net/pdf?id=ElaajXDEKR)
> * Generate text interleaved with retrieved images.
11. Retrieval-Augmented Multimodal Language Modeling.[Link](https://openreview.net/pdf?id=VZ8bs0fwoO)
> * Enables a base multimodal model (generator) to refer to relevant text and images fetched by a retriever from external 

## 2022

1. Mitigating Modality Collapse in Multimodal VAEs via Impartial Optimization.[Link](https://proceedings.mlr.press/v162/javaloy22a/javaloy22a.pdf)
> * Multimodal VAE training
2. Geometric Multimodal Contrastive Representation Learning.[Link](https://proceedings.mlr.press/v162/poklukar22a.html)
3. Ofa: Unifying architectures, tasks, and modalities through a simple sequence-to-sequence learning framework.[Link](https://proceedings.mlr.press/v162/wang22al.html)
> * This work pursues a unified paradigm for multimodal pretraining to break the shackles of complex task/modality-specific customization.

# ICLR

## 2024
1. Benchmarking Multimodal Variational Autoencoders: CdSprites+ Dataset and Toolkit.[Link](https://openreview.net/attachment?id=3DPTnFokLp&name=pdf)
> * Benchmark comparison and evaluation for Multimodal VAE
2. Score-Based Multimodal Autoencoders.[Link](https://openreview.net/attachment?id=YBSEwwveMr&name=pdf)
> * enhance the generative performance of multimodal VAEs

## 2023

1. MultiViz: Towards Visualizing and Understanding Multimodal Models.[Link](https://openreview.net/pdf?id=i2_TvOFmEml)
> * Assign interpretable concepts to features
2. Multimodal Analogical Reasoning over Knowledge Graphs.[Link](https://arxiv.org/pdf/2210.00312.pdf)
> * KG
3. Socratic Models: Composing Zero-Shot Multimodal Reasoning with Language.[Link](https://arxiv.org/pdf/2204.00598.pdf)
> * Socratic Models (SMs): a modular framework in which multiple pretrained models may be composed zero-shot.
4. Meta Learning to Bridge Vision and Language Models for Multimodal Few-Shot Learning.[Link](https://arxiv.org/pdf/2302.14794.pdf)
> * Multimodal few-shot learning
5. MMVAE+: Enhancing the Generative Quality of Multimodal VAEs without Compromises.[Link](https://www.research-collection.ethz.ch/handle/20.500.11850/637761)
> * Multimodal VAEs
6. Identifiability Results for Multimodal Contrastive Learning.[Link](https://arxiv.org/pdf/2303.09166.pdf)
7. Multimodal Federated Learning via Contrastive Representation Ensemble[Link](https://arxiv.org/pdf/2302.08888.pdf)
8. Learning Multimodal Data Augmentation in Feature Space.[Link](https://arxiv.org/pdf/2212.14453.pdf)

## 2022
1. Learning Multimodal VAEs through Mutual Supervision.[Link](https://arxiv.org/pdf/2106.12570.pdf)
> * Multimodal VAEs
2. On the Limitations of Multimodal VAEs.[Link](https://openreview.net/forum?id=w-CPUXXrAj)
> * Multimodal VAEs
3. Learning Audio-Visual Speech Representation by Masked Multimodal Cluster Prediction.[Link](https://arxiv.org/pdf/2201.02184.pdf)

## 2021
1. Generalized Multimodal ELBO.[Link](https://openreview.net/forum?id=5Y21V0RDBV)
2. MultiModalQA: complex question answering over text, tables and images.[Link](https://openreview.net/forum?id=ee6W5UgQLa)
3. HalentNet: Multimodal Trajectory Forecasting with Hallucinative Intents.[Link](https://openreview.net/forum?id=9GBZBPn0Jx)
4. Parameter Efficient Multimodal Transformers for Video Representation Learning.[Link](https://openreview.net/forum?id=6UdQLhqJyFD)
5. Relating by Contrasting: A Data-efficient Framework for Multimodal Generative Models.[Link](https://openreview.net/forum?id=vhKe9UFbrJo)
> * Reduce the amount of related data required for effective learning



# NeurIPS
## 2023
Injecting Multimodal Information into Rigid Protein Docking via Bi-level Optimization[[link](https://openreview.net/pdf?id=ZuaVKlWdD2)]

[DATASET]M3Exam: A Multilingual, Multimodal, Multilevel Benchmark for Examining Large Language Models[[link](https://openreview.net/pdf?id=hJPATsBb3l)]

Alternating Gradient Descent and Mixture-of-Experts for Integrated Multimodal Perception[[link](https://openreview.net/pdf?id=uTlKUAm68H)]

Mass-Producing Failures of Multimodal Systems with Language Models[[link](https://openreview.net/pdf?id=T6iiOqsGOh)]

VLATTACK: Multimodal Adversarial Attacks on Vision-Language Tasks via Pre-trained Models[[link](https://openreview.net/pdf?id=qBAED3u1XZ)]

RH-BrainFS: Regional Heterogeneous Multimodal Brain Networks Fusion Strategy[[link](https://openreview.net/pdf?id=s97ezbqoDZ)]

Integration-free Training for Spatio-temporal Multimodal Covariate Deep Kernel Point Processes[[link](https://openreview.net/pdf?id=Yvpenkym8A)]

Module-wise Adaptive Distillation for Multimodality Foundation Models[[link](https://openreview.net/pdf?id=JhQP33aMx2)]

Guide Your Agent with Adaptive Multimodal Rewards[[link](https://openreview.net/pdf?id=G8nal7MpIQ)]

DDCoT: Duty-Distinct Chain-of-Thought Prompting for Multimodal Reasoning in Language Models[[link](https://openreview.net/pdf?id=ktYjrgOENR)]

A Theory of Multimodal Learning[[link](https://openreview.net/pdf?id=7xlrdSOm3g)]

Multimodal Deep Learning Model Unveils Behavioral Dynamics of V1 Activity in Freely Moving Mice[[link](https://openreview.net/pdf?id=qv5UZJTNda)]

Foundation Model is Efficient Multimodal Multitask Model Selector[[link](https://openreview.net/pdf?id=2ep5PXEZiw)]

Into the LAION’s Den: Investigating Hate in Multimodal Datasets[[link](https://openreview.net/pdf?id=6URyQ9QhYv)]

MuSe-GNN: Learning Unified Gene Representation From Multimodal Biological Graph Data[[link](https://openreview.net/pdf?id=4UCktT9XZx)]

ASIF: Coupled Data Turns Unimodal Models to Multimodal without Training[[link](https://openreview.net/pdf?id=XjOj3ZmWEl)]

[DATASET]Multimodal Clinical Benchmark for Emergency Care (MC-BEC): A Comprehensive Benchmark for Evaluating Foundation Models in Emergency Medicine[[link](https://openreview.net/pdf?id=aKnWIrDPiR)]

SPAE: Semantic Pyramid AutoEncoder for Multimodal Generation with Frozen LLMs[[link](https://openreview.net/pdf?id=CXPUg86A1D)]

[DATASET]Intelligent Knee Sleeves: A Real-time Multimodal Dataset for 3D Lower Body Motion Estimation Using Smart Textile[[link](https://openreview.net/pdf?id=T3FKjN4p8d)]

[DATASET]DATACOMP: In search of the next generation of multimodal datasets[[link](https://openreview.net/pdf?id=dVaWCDMBof)]

[DATASET]Multimodal C4: An Open, Billion-scale Corpus of Images Interleaved with Text[[link](https://openreview.net/pdf?id=tOd8rSjcWz)]

Generating Images with Multimodal Language Models[[link](https://openreview.net/pdf?id=Uczck6TlSZ)]

Quantifying & Modeling Multimodal Interactions: An Information Decomposition Framework[[link](https://openreview.net/pdf?id=J1gBijopla)]

[DATASET]YouTubePD: A Multimodal Benchmark for Parkinson’s Disease Analysis[[link](https://openreview.net/pdf?id=AIeeXKsspI)]

[DATASET]Improving multimodal datasets with image captioning[[link](https://openreview.net/pdf?id=VIRKdeFJIg)]

MultiModN—Multimodal, Multi-Task, Interpretable Modular Networks[[link](https://openreview.net/pdf?id=iB3Ew6z4WL)]

[DATASET]Perception Test: A Diagnostic Benchmark for Multimodal Video Models[[link](https://openreview.net/forum?id=HYEGXFnPoq)]

[SPOTLIGHT]4M: Massively Multimodal Masked Modeling[[link](https://openreview.net/pdf?id=TegmlsD8oQ)]

[DATASET]HOH: Markerless Multimodal Human-Object-Human Handover Dataset with Large Object Count[[link](https://openreview.net/pdf?id=7bghy0Gq75)]

Beyond Unimodal: Generalising Neural Processes for Multimodal Uncertainty Estimation[[link](https://openreview.net/pdf?id=xq1QvViDdW)]

[DATASET]Learning to Taste : A Multimodal Wine Dataset[[link](https://openreview.net/pdf?id=VeJgZYhT7H)]

Incomplete Multimodality-Diffused Emotion Recognition[[link](https://openreview.net/pdf?id=BuGFwUS9B3)]

RegBN: Batch Normalization of Multimodal Data with Regularization[[link](https://openreview.net/pdf?id=nUbdkXqC8R)]

[DATASET]ProBio: A Protocol-guided Multimodal Dataset for Molecular Biology Lab[[link](https://openreview.net/pdf?id=846X3N11bf)]

[DATASET]INSPECT: A Multimodal Dataset for Pulmonary Embolism Diagnosis and Prognosis[[link](https://openreview.net/pdf?id=3sRR2u72oQ)]

Parameter-efficient Tuning of Large-scale Multimodal Foundation Model[[link](https://openreview.net/pdf?id=pT8DIhsJCw)]

[TIME SERIES]FOCAL: Contrastive Learning for Multimodal Time-Series Sensing Signals in Factorized Orthogonal Latent Space[[link](https://openreview.net/pdf?id=l4CZCKXoSn)]

Achieving Cross Modal Generalization with Multimodal Unified Representation[[link](https://openreview.net/forum?id=t7ZowrDWVw)]

[DATASET]StressID: a Multimodal Dataset for Stress Identification[[link](https://openreview.net/pdf?id=qWsQi9DGJb)]

Open Visual Knowledge Extraction via Relation-Oriented Multimodality Model Prompting[[link](https://openreview.net/pdf?id=ixVAXsdtJO)]

[DATASET]AircraftVerse: A Large-Scale Multimodal Dataset of Aerial Vehicle Designs[[link](https://openreview.net/pdf?id=MfhJWSp3Ea)]

Implicit Differentiable Outlier Detection Enables Robust Deep Multimodal Analysis[[link](https://openreview.net/pdf?id=jooPcatnVF)]

Training Transitive and Commutative Multimodal Transformers with LoReTTa[[link](https://openreview.net/pdf?id=nArzDm353Y)]

Brain encoding models based on multimodal transformers can transfer across language and vision[[link](https://openreview.net/pdf?id=UPefaFqjNQ)]

## 2022
Multi-modal Grouping Network for Weakly-Supervised Audio-Visual Video Parsing[[link](https://proceedings.neurips.cc/paper_files/paper/2022/file/e095c0a3717629aa5497601985bfcf0e-Paper-Conference.pdf)]

Mind the Gap: Understanding the Modality Gap in Multi-modal Contrastive Representation Learning[[link](https://proceedings.neurips.cc/paper_files/paper/2022/file/702f4db7543a7432431df588d57bc7c9-Paper-Conference.pdf)]

M4I: Multi-modal Models Membership Inference[[link](https://proceedings.neurips.cc/paper_files/paper/2022/file/0c79d6ed1788653643a1ac67b6ea32a7-Paper-Conference.pdf)]

OTKGE: Multi-modal Knowledge Graph Embeddings via Optimal Transport[[link](https://proceedings.neurips.cc/paper_files/paper/2022/file/ffdb280e7c7b4c4af30e04daf5a84b98-Paper-Conference.pdf)]

Deep Multi-Modal Structural Equations For Causal Effect Estimation With Unstructured Proxies[[link](Deep Multi-Modal Structural Equations For Causal Effect Estimation With Unstructured Proxies)]

mRI: Multi-modal 3D Human Pose Estimation Dataset using mmWave, RGB-D, and Inertial Sensors[[link](https://proceedings.neurips.cc/paper_files/paper/2022/file/af9c9c6d2da701da5a0acf91ec217815-Paper-Datasets_and_Benchmarks.pdf)]

u-HuBERT: Unified Mixed-Modal Speech Pretraining And Zero-Shot Transfer to Unlabeled Modality[[link](https://proceedings.neurips.cc/paper_files/paper/2022/file/853e781cb2af58956ed5c89aa59da3fc-Paper-Conference.pdf)]

CHIMLE: Conditional Hierarchical IMLE for Multimodal Conditional Image Synthesis[[link](https://proceedings.neurips.cc/paper_files/paper/2022/file/0207c9ea9faf66c6e892c3fa3c167b75-Paper-Conference.pdf)]

Transferring Pre-trained Multimodal Representations with Cross-modal Similarity Matching[[link](https://proceedings.neurips.cc/paper_files/paper/2022/file/c7201deff8d507a8fe2e86d34094e154-Paper-Conference.pdf)]

Multi-Lingual Acquisition on Multimodal Pre-training for Cross-modal Retrieval[[link](https://proceedings.neurips.cc/paper_files/paper/2022/file/bfadef437ed27372648714c930c3a77a-Paper-Conference.pdf)]

Can Push-forward Generative Models Fit Multimodal Distributions?[[link](https://proceedings.neurips.cc/paper_files/paper/2022/file/45f0d179ef7e10eb7366550cd4e574ae-Paper-Conference.pdf)]

CAESAR: An Embodied Simulator for Generating Multimodal Referring Expression Datasets[[link](https://proceedings.neurips.cc/paper_files/paper/2022/file/844f722dbbcb27933ff5baf58a1f00c8-Paper-Datasets_and_Benchmarks.pdf)]

Long-Form Video-Language Pre-Training with Multimodal Temporal Contrastive Learning[[link](https://proceedings.neurips.cc/paper_files/paper/2022/file/f8290ccc2905538be1a7f7914ccef629-Paper-Conference.pdf)]

VLMO: Unified Vision-Language Pre-Training with Mixture-of-Modality-Experts[[link](https://proceedings.neurips.cc/paper_files/paper/2022/file/d46662aa53e78a62afd980a29e0c37ed-Paper-Conference.pdf)]

Learn to Explain: Multimodal Reasoning via Thought Chains for Science Question Answering[[link](https://proceedings.neurips.cc/paper_files/paper/2022/file/11332b6b6cf4485b84afadb1352d3a9a-Paper-Conference.pdf)]

Mutual Information Divergence: A Unified Metric for Multimodal Generative Models[[link](https://proceedings.neurips.cc/paper_files/paper/2022/file/e40b60677880e7e74f8a081f65703f0d-Paper-Conference.pdf)]

Let Images Give You More: Point Cloud Cross-Modal Training for Shape Analysis[[link](https://neurips.cc/virtual/2022/poster/55376)]

On the Effect of Pre-training for Transformer in Different Modality on Offline Reinforcement Learning[[link](https://neurips.cc/virtual/2022/poster/55028)]

Wukong: A 100 Million Large-scale Chinese Cross-modal Pre-training Benchmark[[link](https://neurips.cc/virtual/2022/poster/55733)]

A Differentiable Semantic Metric Approximation in Probabilistic Embedding for Cross-Modal Retrieval[[link](https://neurips.cc/virtual/2022/poster/54353)]

Scaling Multimodal Pre-Training via Cross-Modality Gradient Harmonization[[link](https://neurips.cc/virtual/2022/poster/53966)]

Cross-Linked Unified Embedding for cross-modality representation learning[[link](https://neurips.cc/virtual/2022/poster/53026)]

Kernel Multimodal Continuous Attention[[link](https://neurips.cc/virtual/2022/poster/53139)]

Co-Modality Graph Contrastive Learning for Imbalanced Node Classification[[link](https://neurips.cc/virtual/2022/poster/53806)]

MACK: Multimodal Aligned Conceptual Knowledge for Unpaired Image-text Matching[[link](https://neurips.cc/virtual/2022/poster/53179)]

Multimodal Contrastive Learning with LIMoE: the Language-Image Mixture of Experts[[link](https://neurips.cc/virtual/2022/poster/54648)]

Towards Effective Multi-Modal Interchanges in Zero-Resource Sounding Object Localization[[link](https://neurips.cc/virtual/2022/poster/54588)]

ActionSense: A Multimodal Dataset and Recording Framework for Human Activities Using Wearable Sensors in a Kitchen Environment[[link](https://neurips.cc/virtual/2022/poster/55638)]

Multi-Granularity Cross-modal Alignment for Generalized Medical Visual Representation Learning[[link](https://neurips.cc/virtual/2022/poster/55023)]

Deep Multi-Modal Structural Equations For Causal Effect Estimation With Unstructured Proxies[[link](https://neurips.cc/virtual/2022/poster/54852)]

Cross-modal Learning for Image-Guided Point Cloud Shape Completion[[link](https://neurips.cc/virtual/2022/poster/54809)]

# Workshops & Tutorial
1. Proceedings of the Workshop on Multi-Modal Fake News and Hate-Speech Detection (DE-FACTIFY 2022) co-located with the Thirty-Sixth AAAI Conference on Artificial Intelligence ( AAAI 2022), Virtual Event, Vancouver, Canada, February 27, 2022. CEUR Workshop Proceedings 3199, CEUR-WS.org 2022
