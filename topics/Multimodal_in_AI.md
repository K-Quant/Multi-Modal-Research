# AAAI 
## 2023
1. MMTN: Multi-Modal Memory Transformer Network for Image-Report Consistent Medical Report Generation.[Link](https://ojs.aaai.org/index.php/AAAI/article/view/25100)
> * Multi-modal medical data 
3. Tagging before Alignment: Integrating Multi-Modal Tags for Video-Text Retrieval.[Link](https://ojs.aaai.org/index.php/AAAI/article/view/25113)
> *  Video-text retrieval task
> *  A joint cross-modal encoder with the triplet input of [vision, tag, text] and perform two additional supervised tasks, Video Text Matching (VTM) and Masked Language Modeling (MLM).
4. Open-Vocabulary Multi-Label Classification via Multi-Modal Knowledge Transfer.
5. PATRON: Perspective-Aware Multitask Model for Referring Expression Grounding Using Embodied Multimodal Cues.[Link](https://ojs.aaai.org/index.php/AAAI/article/view/25177)
6. Multi-Modality Deep Network for Extreme Learned Image Compression.[Link](https://ojs.aaai.org/index.php/AAAI/article/view/25184)
> * Text-Image 
> * Adopt the image-text attention module and image-request complement module to better fuse image and text features
7. Learning Polysemantic Spoof Trace: A Multi-Modal Disentanglement Network for Face Anti-spoofing.[Link](https://ojs.aaai.org/index.php/AAAI/article/view/25219)
8. M3AE: Multimodal Representation Learning for Brain Tumor Segmentation with Missing Modalities. 
9. Efficient End-to-End Video Question Answering with Pyramidal Multimodal Transformer. 
10. Just Noticeable Visual Redundancy Forecasting: A Deep Multimodal-Driven Approach.[Link](https://ojs.aaai.org/index.php/AAAI/article/view/25399)
> * Homologous multimodal information: saliency, depth, and segmentation.  
11. Multi-Modal Knowledge Hypergraph for Diverse Image Retrieval.[Link](https://ojs.aaai.org/index.php/AAAI/article/view/25445)
> *  keyword-based diverse image retrieval
12. TOT：Topology-Aware Optimal Transport for Multimodal Hate Detection.[Link](https://ojs.aaai.org/index.php/AAAI/article/view/25614)
13. Causal Conditional Hidden Markov Model for Multimodal Traffic Prediction.[Link](https://ojs.aaai.org/index.php/AAAI/article/view/25619)
> * Multi-modal traffic flow
14. Heterogeneous Graph Learning for Multi-Modal Medical Data Analysis.[Link](https://ojs.aaai.org/index.php/AAAI/article/view/25643)
> * Multi-modal medical data 
15. Sparse Maximum Margin Learning from Multimodal Human Behavioral Patterns.[Link](https://ojs.aaai.org/index.php/AAAI/article/view/25676)
16. MNER-QG: An End-to-End MRC Framework for Multimodal Named Entity Recognition with Query Grounding.[Link](https://ojs.aaai.org/index.php/AAAI/article/view/25971)
> * Text-Image
17. Mutual-Enhanced Incongruity Learning Network for Multi-Modal Sarcasm Detection.[Link](https://ojs.aaai.org/index.php/AAAI/article/view/26138)
18. Accommodating Audio Modality in CLIP for Multimodal Processing. [Link](https://ojs.aaai.org/index.php/AAAI/article/view/26153)
> * Vision-Language-Audio multimodal
19. Self-Supervised Audio-Visual Representation Learning with Relaxed Cross-Modal Synchronicity.[Link](https://ojs.aaai.org/index.php/AAAI/article/view/26162)
20. i-Code: An Integrative and Composable Multimodal Learning Framework. [Link](https://ojs.aaai.org/index.php/AAAI/article/view/26290)
21. Joint Multimodal Entity-Relation Extraction Based on Edge-Enhanced Graph Alignment Network and Word-Pair Relation Tagging. [Link](https://ojs.aaai.org/index.php/AAAI/article/view/26309)
22. Multi-Level Confidence Learning for Trustworthy Multimodal Classification. [Link]()
23. Bayesian Cross-Modal Alignment Learning for Few-Shot Out-of-Distribution Generalization. [Link](https://ojs.aaai.org/index.php/AAAI/article/view/26355)
24. Cross-Modal Distillation for Speaker Recognition.[Link](https://ojs.aaai.org/index.php/AAAI/article/view/26525)
>   * Face image & Speech
25. Explaining (Sarcastic) Utterances to Enhance Affect Understanding in Multimodal Dialogues. [Link](https://ojs.aaai.org/index.php/AAAI/article/view/26526)
26. SPRING: Situated Conversation Agent Pretrained with Multimodal Questions from Incremental Layout Graph. [Link](https://doi.org/10.1609/aaai.v37i11.26562)
> * Complex relative positions and information alignments
27. See How You Read? Multi-Reading Habits Fusion Reasoning for Multi-Modal Fake News Detection.[Link](https://ojs.aaai.org/index.php/AAAI/article/view/26609)
28. MPMQA: Multimodal Question Answering on Product Manuals.[Link](https://ojs.aaai.org/index.php/AAAI/article/view/26634)
29. FakeSV: A Multimodal Benchmark with Rich Social Context for Fake News Detection on Short Video Platforms.[Link](https://ojs.aaai.org/index.php/AAAI/article/view/26689)
30. MuMIC - Multimodal Embedding for Multi-Label Image Classification with Tempered Sigmoid.[Link](https://ojs.aaai.org/index.php/AAAI/article/view/26850)
31. Multimodal Propaganda Processing.[Link](https://ojs.aaai.org/index.php/AAAI/article/view/26792)

## 2022
1. Event-Image Fusion Stereo Using Cross-Modality Feature Propagation.[Link](https://ojs.aaai.org/index.php/AAAI/article/view/19923)
> * Image & Event
2. MuMu: Cooperative Multitask Learning-Based Guided Multimodal Fusion.[Link](https://ojs.aaai.org/index.php/AAAI/article/view/19988)_
> * Multimodal sensors (visual, non-visual, and wearable)
3. Cross-Modal Object Tracking: Modality-Aware Representations and a Unified Benchmark.[Link](https://ojs.aaai.org/index.php/AAAI/article/view/20016)
4. You Only Infer Once: Cross-Modal Meta-Transfer for Referring Video Object Segmentation.[Link](https://ojs.aaai.org/index.php/AAAI/article/view/20017)
> * Texts & Images
5. Multi-Modal Perception Attention Network with Self-Supervised Learning for Audio-Visual Speaker Tracking.[Link](https://ojs.aaai.org/index.php/AAAI/article/view/20035)
6. Visual Sound Localization in the Wild by Cross-Modal Interference Erasing.[Link](https://ojs.aaai.org/index.php/AAAI/article/view/20073)
7. TVT: Three-Way Vision Transformer through Multi-Modal Hypersphere Learning for Zero-Shot Sketch-Based Image Retrieval.[Link](https://ojs.aaai.org/index.php/AAAI/article/view/20136)
8. Interact, Embed, and EnlargE: Boosting Modality-Specific Representations for Multi-Modal Person Re-identification.[Link](https://ojs.aaai.org/index.php/AAAI/article/view/20165)
9. Multi-Modal Answer Validation for Knowledge-Based VQA. [Link](https://ojs.aaai.org/index.php/AAAI/article/view/20174)
10. Cross-Modal Mutual Learning for Audio-Visual Speech Recognition and Manipulation. [Link](https://ojs.aaai.org/index.php/AAAI/article/view/20210)
> * Visual & Audio
11. Cross-Modal Federated Human Activity Recognition via Modality-Agnostic and Modality-Specific Representation Learning.[Link](https://ojs.aaai.org/index.php/AAAI/article/view/20213)
12. Show Your Faith: Cross-Modal Confidence-Aware Network for Image-Text Matching.[Link](https://ojs.aaai.org/index.php/AAAI/article/view/20235)
> * Texts & Images
13. MAGIC: Multimodal relAtional Graph adversarIal inferenCe for Diverse and Unpaired Text-Based Image Captioning.[Link](https://ojs.aaai.org/index.php/AAAI/article/view/20243)
> * Texts & Images
14. Event-Aware Multimodal Mobility Nowcasting.[Link](https://ojs.aaai.org/index.php/AAAI/article/view/20342)
> * Event-aware spatio-temporal network
15. Online Enhanced Semantic Hashing: Towards Effective and Efficient Retrieval for Streaming Multi-Modal Data.[Link](https://ojs.aaai.org/index.php/AAAI/article/view/20346)
16. Multimodal Adversarially Learned Inference with Factorized Discriminators.[Link](https://ojs.aaai.org/index.php/AAAI/article/view/20580)
17. Multi-Head Modularization to Leverage Generalization Capability in Multi-Modal Networks. [Link](https://ojs.aaai.org/index.php/AAAI/article/view/20698)
18. BM-NAS: Bilevel Multimodal Neural Architecture Search. [Link](https://ojs.aaai.org/index.php/AAAI/article/view/20872)
19. Tailor Versatile Multi-Modal Learning for Multi-Label Emotion Recognition.[Link](https://ojs.aaai.org/index.php/AAAI/article/view/20895)
> * Visual, Audio and Text 
20. Are Vision-Language Transformers Learning Multimodal Representations? A Probing Perspective.[Link](https://ojs.aaai.org/index.php/AAAI/article/view/21375)
> * Texts & Images
21. Hierarchical Cross-Modality Semantic Correlation Learning Model for Multimodal Summarization. [Link](https://ojs.aaai.org/index.php/AAAI/article/view/21422)
> * Texts & Images
22. UniMS: A Unified Framework for Multimodal Summarization with Knowledge Distillation.[Link](https://ojs.aaai.org/index.php/AAAI/article/view/21431)
> * Texts & Images
23. Evaluating Explainable AI on a Multi-Modal Medical Imaging Task: Can Existing Algorithms Fulfill Clinical Requirements?[Link](https://ojs.aaai.org/index.php/AAAI/article/view/21452)
24. Sentiment and Emotion-Aware Multi-Modal Complaint Identification. [Link](https://ojs.aaai.org/index.php/AAAI/article/view/21476)
> * Texts & Images
25. D-vlog: Multimodal Vlog Dataset for Depression Detection.[Link](https://ojs.aaai.org/index.php/AAAI/article/view/21483)
> * Acoustic & Visual freatures
# Workshops & Tutorial
1. Proceedings of the Workshop on Multi-Modal Fake News and Hate-Speech Detection (DE-FACTIFY 2022) co-located with the Thirty-Sixth AAAI Conference on Artificial Intelligence ( AAAI 2022), Virtual Event, Vancouver, Canada, February 27, 2022. CEUR Workshop Proceedings 3199, CEUR-WS.org 2022
2.
3. 
# ICML
## 2023


1. Provable Dynamic Fusion for Low-Quality Multimodal Data.[Link](https://arxiv.org/pdf/2306.02050.pdf)
> * Introducing dynamic fusion mechanism to multimodal information fusion 
2. Calibrating Multimodal Learning.[Link](https://openreview.net/pdf?id=4PgzyLz6hi)
> * A novel regularization technique to calibrate the predictive confidence of previous methods.
3. π-Tuning: Transferring Multimodal Foundation Models with Optimal Multi-task Interpolation.[Link](https://proceedings.mlr.press/v202/wu23t/wu23t.pdf)
4. Improving Medical Predictions by Irregular Multimodal Electronic Health Records Modeling.[Link](https://proceedings.mlr.press/v202/zhang23v/zhang23v.pdf)
>* Medical Prediction
5. MEWL: Few-shot multimodal word learning with referential uncertainty.[Link](https://arxiv.org/pdf/2306.00503.pdf)
>* This paper introduces the MachinE Word Learning (MEWL) benchmark to assess how machines learn word meaning in grounded visual scenes.
6. VIMA: Robot Manipulation with Multimodal Prompts.[Link](https://proceedings.mlr.press/v202/jiang23b/jiang23b.pdf)
> * Multimodal application in generalist robot
7. Robustness in Multimodal Learning under Train-Test Modality Mismatch.[Link](https://proceedings.mlr.press/v202/mckinzie23a/mckinzie23a.pdf)
> * Robustness improvement in multimodal learning
8. Data Poisoning Attacks Against Multimodal Encoders.[Link](https://proceedings.mlr.press/v202/yang23f/yang23f.pdf)
> * Anti-attack in multimodal encoders
9. PaLM-E: An Embodied Multimodal Language Model.[Link](https://proceedings.mlr.press/v202/driess23a/driess23a.pdf)
> * This paper proposes embodied language models to directly incorporate real-world continuous sensor modalities into language models and thereby establish the link between words and percepts.
10. Grounding Language Models to Images for Multimodal Inputs and Outputs.[Link](https://openreview.net/pdf?id=ElaajXDEKR)
> * Generate text interleaved with retrieved images.
11. Retrieval-Augmented Multimodal Language Modeling.[Link](https://openreview.net/pdf?id=VZ8bs0fwoO)
> * Enables a base multimodal model (generator) to refer to relevant text and images fetched by a retriever from external 

## 2022

1. Mitigating Modality Collapse in Multimodal VAEs via Impartial Optimization.[Link](https://proceedings.mlr.press/v162/javaloy22a/javaloy22a.pdf)
> * Multimodal VAE training
2. Geometric Multimodal Contrastive Representation Learning.[Link](https://proceedings.mlr.press/v162/poklukar22a.html)
3. Ofa: Unifying architectures, tasks, and modalities through a simple sequence-to-sequence learning framework.[Link](https://proceedings.mlr.press/v162/wang22al.html)
> * This work pursues a unified paradigm for multimodal pretraining to break the shackles of complex task/modality-specific customization.

# ICLR

## 2024
1. Benchmarking Multimodal Variational Autoencoders: CdSprites+ Dataset and Toolkit.[Link](https://openreview.net/attachment?id=3DPTnFokLp&name=pdf)
> * Benchmark comparison and evaluation for Multimodal VAE
2. Score-Based Multimodal Autoencoders.[Link](https://openreview.net/attachment?id=YBSEwwveMr&name=pdf)
> * enhance the generative performance of multimodal VAEs

## 2023

1. MultiViz: Towards Visualizing and Understanding Multimodal Models.[Link](https://openreview.net/pdf?id=i2_TvOFmEml)
> * Assign interpretable concepts to features
2. Multimodal Analogical Reasoning over Knowledge Graphs.[Link](https://arxiv.org/pdf/2210.00312.pdf)
> * KG
3. Socratic Models: Composing Zero-Shot Multimodal Reasoning with Language.[Link](https://arxiv.org/pdf/2204.00598.pdf)
> * Socratic Models (SMs): a modular framework in which multiple pretrained models may be composed zero-shot.
4. Meta Learning to Bridge Vision and Language Models for Multimodal Few-Shot Learning.[Link](https://arxiv.org/pdf/2302.14794.pdf)
> * Multimodal few-shot learning
5. MMVAE+: Enhancing the Generative Quality of Multimodal VAEs without Compromises.[Link](https://www.research-collection.ethz.ch/handle/20.500.11850/637761)
> * Multimodal VAEs
6. Identifiability Results for Multimodal Contrastive Learning.[Link](https://arxiv.org/pdf/2303.09166.pdf)
7. Multimodal Federated Learning via Contrastive Representation Ensemble[Link](https://arxiv.org/pdf/2302.08888.pdf)
8. Learning Multimodal Data Augmentation in Feature Space.[Link](https://arxiv.org/pdf/2212.14453.pdf)

## 2022
1. Learning Multimodal VAEs through Mutual Supervision.[Link](https://arxiv.org/pdf/2106.12570.pdf)
> * Multimodal VAEs
2. On the Limitations of Multimodal VAEs.[Link](https://openreview.net/forum?id=w-CPUXXrAj)
> * Multimodal VAEs
3. Learning Audio-Visual Speech Representation by Masked Multimodal Cluster Prediction.[Link](https://arxiv.org/pdf/2201.02184.pdf)

## 2021
1. Generalized Multimodal ELBO.[Link](https://openreview.net/forum?id=5Y21V0RDBV)
2. MultiModalQA: complex question answering over text, tables and images.[Link](https://openreview.net/forum?id=ee6W5UgQLa)
3. HalentNet: Multimodal Trajectory Forecasting with Hallucinative Intents.[Link](https://openreview.net/forum?id=9GBZBPn0Jx)
4. Parameter Efficient Multimodal Transformers for Video Representation Learning.[Link](https://openreview.net/forum?id=6UdQLhqJyFD)
5. Relating by Contrasting: A Data-efficient Framework for Multimodal Generative Models.[Link](https://openreview.net/forum?id=vhKe9UFbrJo)
> * Reduce the amount of related data required for effective learning



# NeurIPS
## 2023
Injecting Multimodal Information into Rigid Protein Docking via Bi-level Optimization[[link](https://openreview.net/pdf?id=ZuaVKlWdD2)]

[DATASET]M3Exam: A Multilingual, Multimodal, Multilevel Benchmark for Examining Large Language Models[[link](https://openreview.net/pdf?id=hJPATsBb3l)]

Alternating Gradient Descent and Mixture-of-Experts for Integrated Multimodal Perception[[link](https://openreview.net/pdf?id=uTlKUAm68H)]

Mass-Producing Failures of Multimodal Systems with Language Models[[link](https://openreview.net/pdf?id=T6iiOqsGOh)]

VLATTACK: Multimodal Adversarial Attacks on Vision-Language Tasks via Pre-trained Models[[link](https://openreview.net/pdf?id=qBAED3u1XZ)]

RH-BrainFS: Regional Heterogeneous Multimodal Brain Networks Fusion Strategy[[link](https://openreview.net/pdf?id=s97ezbqoDZ)]

Integration-free Training for Spatio-temporal Multimodal Covariate Deep Kernel Point Processes[[link](https://openreview.net/pdf?id=Yvpenkym8A)]

Module-wise Adaptive Distillation for Multimodality Foundation Models[[link](https://openreview.net/pdf?id=JhQP33aMx2)]

Guide Your Agent with Adaptive Multimodal Rewards[[link](https://openreview.net/pdf?id=G8nal7MpIQ)]

DDCoT: Duty-Distinct Chain-of-Thought Prompting for Multimodal Reasoning in Language Models[[link](https://openreview.net/pdf?id=ktYjrgOENR)]

A Theory of Multimodal Learning[[link](https://openreview.net/pdf?id=7xlrdSOm3g)]

Multimodal Deep Learning Model Unveils Behavioral Dynamics of V1 Activity in Freely Moving Mice[[link](https://openreview.net/pdf?id=qv5UZJTNda)]

Foundation Model is Efficient Multimodal Multitask Model Selector[[link](https://openreview.net/pdf?id=2ep5PXEZiw)]

Into the LAION’s Den: Investigating Hate in Multimodal Datasets[[link](https://openreview.net/pdf?id=6URyQ9QhYv)]

MuSe-GNN: Learning Unified Gene Representation From Multimodal Biological Graph Data[[link](https://openreview.net/pdf?id=4UCktT9XZx)]

ASIF: Coupled Data Turns Unimodal Models to Multimodal without Training[[link](https://openreview.net/pdf?id=XjOj3ZmWEl)]

[DATASET]Multimodal Clinical Benchmark for Emergency Care (MC-BEC): A Comprehensive Benchmark for Evaluating Foundation Models in Emergency Medicine[[link](https://openreview.net/pdf?id=aKnWIrDPiR)]

SPAE: Semantic Pyramid AutoEncoder for Multimodal Generation with Frozen LLMs[[link](https://openreview.net/pdf?id=CXPUg86A1D)]

[DATASET]Intelligent Knee Sleeves: A Real-time Multimodal Dataset for 3D Lower Body Motion Estimation Using Smart Textile[[link](https://openreview.net/pdf?id=T3FKjN4p8d)]

[DATASET]DATACOMP: In search of the next generation of multimodal datasets[[link](https://openreview.net/pdf?id=dVaWCDMBof)]

[DATASET]Multimodal C4: An Open, Billion-scale Corpus of Images Interleaved with Text[[link](https://openreview.net/pdf?id=tOd8rSjcWz)]

Generating Images with Multimodal Language Models[[link](https://openreview.net/pdf?id=Uczck6TlSZ)]

Quantifying & Modeling Multimodal Interactions: An Information Decomposition Framework[[link](https://openreview.net/pdf?id=J1gBijopla)]

[DATASET]YouTubePD: A Multimodal Benchmark for Parkinson’s Disease Analysis[[link](https://openreview.net/pdf?id=AIeeXKsspI)]

[DATASET]Improving multimodal datasets with image captioning[[link](https://openreview.net/pdf?id=VIRKdeFJIg)]

MultiModN—Multimodal, Multi-Task, Interpretable Modular Networks[[link](https://openreview.net/pdf?id=iB3Ew6z4WL)]

[DATASET]Perception Test: A Diagnostic Benchmark for Multimodal Video Models[[link](https://openreview.net/forum?id=HYEGXFnPoq)]

[SPOTLIGHT]4M: Massively Multimodal Masked Modeling[[link](https://openreview.net/pdf?id=TegmlsD8oQ)]

[DATASET]HOH: Markerless Multimodal Human-Object-Human Handover Dataset with Large Object Count[[link](https://openreview.net/pdf?id=7bghy0Gq75)]

Beyond Unimodal: Generalising Neural Processes for Multimodal Uncertainty Estimation[[link](https://openreview.net/pdf?id=xq1QvViDdW)]

[DATASET]Learning to Taste : A Multimodal Wine Dataset[[link](https://openreview.net/pdf?id=VeJgZYhT7H)]

Incomplete Multimodality-Diffused Emotion Recognition[[link](https://openreview.net/pdf?id=BuGFwUS9B3)]

RegBN: Batch Normalization of Multimodal Data with Regularization[[link](https://openreview.net/pdf?id=nUbdkXqC8R)]

[DATASET]ProBio: A Protocol-guided Multimodal Dataset for Molecular Biology Lab[[link](https://openreview.net/pdf?id=846X3N11bf)]

[DATASET]INSPECT: A Multimodal Dataset for Pulmonary Embolism Diagnosis and Prognosis[[link](https://openreview.net/pdf?id=3sRR2u72oQ)]

Parameter-efficient Tuning of Large-scale Multimodal Foundation Model[[link](https://openreview.net/pdf?id=pT8DIhsJCw)]

[TIME SERIES]FOCAL: Contrastive Learning for Multimodal Time-Series Sensing Signals in Factorized Orthogonal Latent Space[[link](https://openreview.net/pdf?id=l4CZCKXoSn)]

Achieving Cross Modal Generalization with Multimodal Unified Representation[[link](https://openreview.net/forum?id=t7ZowrDWVw)]

[DATASET]StressID: a Multimodal Dataset for Stress Identification[[link](https://openreview.net/pdf?id=qWsQi9DGJb)]

Open Visual Knowledge Extraction via Relation-Oriented Multimodality Model Prompting[[link](https://openreview.net/pdf?id=ixVAXsdtJO)]

[DATASET]AircraftVerse: A Large-Scale Multimodal Dataset of Aerial Vehicle Designs[[link](https://openreview.net/pdf?id=MfhJWSp3Ea)]

Implicit Differentiable Outlier Detection Enables Robust Deep Multimodal Analysis[[link](https://openreview.net/pdf?id=jooPcatnVF)]

Training Transitive and Commutative Multimodal Transformers with LoReTTa[[link](https://openreview.net/pdf?id=nArzDm353Y)]

Brain encoding models based on multimodal transformers can transfer across language and vision[[link](https://openreview.net/pdf?id=UPefaFqjNQ)]

## 2022
Multi-modal Grouping Network for Weakly-Supervised Audio-Visual Video Parsing[[link](https://proceedings.neurips.cc/paper_files/paper/2022/file/e095c0a3717629aa5497601985bfcf0e-Paper-Conference.pdf)]

Mind the Gap: Understanding the Modality Gap in Multi-modal Contrastive Representation Learning[[link](https://proceedings.neurips.cc/paper_files/paper/2022/file/702f4db7543a7432431df588d57bc7c9-Paper-Conference.pdf)]

M4I: Multi-modal Models Membership Inference[[link](https://proceedings.neurips.cc/paper_files/paper/2022/file/0c79d6ed1788653643a1ac67b6ea32a7-Paper-Conference.pdf)]

OTKGE: Multi-modal Knowledge Graph Embeddings via Optimal Transport[[link](https://proceedings.neurips.cc/paper_files/paper/2022/file/ffdb280e7c7b4c4af30e04daf5a84b98-Paper-Conference.pdf)]

Deep Multi-Modal Structural Equations For Causal Effect Estimation With Unstructured Proxies[[link](Deep Multi-Modal Structural Equations For Causal Effect Estimation With Unstructured Proxies)]


